---
title: "Step Selection Functions"
author: "Kaitlyn"
date: "8/21/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

Explore Step Selection Functions for Hopland hunter data

```{r}
library(tidyverse)
library(lubridate)
library(sf)
library(raster)
library(amt)
library(survival)
library(lmtest)
library(MuMIn)
library(devtools)
library(hab)
```

Import cleaned data file
```{r}
ssfdat.all <- read.csv("Data/ssfdat.all.csv")
```

#### Data exploration

Before we run our models, let's visualize our step lengths and turn angles

Our turn angles are in radians.  For ease of vizualization, we'll turn them into degrees
```{r}
ssfdat.all <- ssfdat.all %>%
  mutate(ta_degree = as_degree(ta_))
```

First, we can look at the distribution of our turning angles as a rose plot
```{r}
ssfdat.all %>% 
  filter(case_ == 1) %>% 
  ggplot(., aes(x = ta_degree, y=..density..)) + geom_histogram(breaks = seq(-180,180, by=20))+
  coord_polar(start = 0) + theme_minimal() + 
  scale_fill_brewer() + ylab("Density") + ggtitle("Angles Direct") + 
  scale_x_continuous("", limits = c(-180, 180), breaks = seq(-180, 180, by=20), 
                     labels = seq(-180, 180, by=20)) #+
 # facet_wrap(~ID) # too crowded when facet-wrapped
```

Also check out the distribution of step lengths
```{r}
ssfdat.all %>% 
  filter(case_ == 1) %>% 
  ggplot(., aes(x = sl_)) +  geom_histogram() + theme_minimal() + 
  scale_fill_brewer() + ylab("Count") + ggtitle("Step Lengths")# +
  #facet_wrap(~ID, scales="free")
```

Let's also look at what used and available steps look like
Here, the black point is our starting location and the colored points are our end locations
```{r}
ssfdat.all %>% 
  filter(step_id_ == 1, ID == 6) %>% 
  ggplot(.) + geom_point(aes(x = x2_,y = y2_,color = as.factor(case_))) + 
  geom_point(aes(x = x1_, y = y1_))
```

Check out some other strata to see how they look by modifying the ID and step_id_


A note on behaviors: You might imagine that animals select habitat really differently if they are in different behavioral states (i.e. resting, feeding, meandering, directed travel). Therefore, it is often wise to seperate your data into states before running SSF analyses.

You can determine the state of your animal in a few different ways. With just GPS data, you can fit a hidden markov model (HMM) to determine state with the moveHMM package. If you have fine-scale accelerometer data, you can also use that to determine state. 


#### Run models

As with RSFs, start by writing out your candidate models.

Here are just a few examples of ways you can structure your models based on different hypotheses. Generally you will use a cluster() term any time you fit SSFs, particulalry if your data are autocorrelated. Therefore, these models below aren't really complete. Here we'll just check them out to get a rough idea of hypothesis testing and model fit

note: amt has a wrapper for clogit called fit_issf. These commands should produce identical outcomes
```{r}
m0 <- clogit(case_ ~ road.dist.clean_end + rugged25.clean_end + vegetation.coarser.clean2_end + 
               strata(stepID),method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)

summary(m0)
```


Do animals select rugged habitats depending on their speed of movement?
m2 has habitat covariates the end with an interaction term between TRI at the start and movement rate
```{r}
m2 <- clogit(case_ ~ road.dist.clean_end + rugged25.clean_end + vegetation.coarser.clean2_end + 
               rugged25.clean_end:log(sl_) +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)

summary(m2)
```


#### Dealing with correlated data

There are multiple ways to integrate correlated data into a single model. The first way is to build in a correlation structure in your model. We can do this by adding a cluster() parameter that seperates your data into clusters of correlated data.

The robust standard errors are now fit using generalized estimating equations (GEE), which correct for autocorrelated data. Note that using GGEs are only appropriate if your data are temporally autocorrelated (see Prima et al. 2017)

Important!!: For GGEs to do an adequate job fitting robust SEs, there need to be enough clusters (>30) (see Prima et al. 2017). Therefore, when we use cluster(), we might want to seperate individual animals into multiple clusters by breaking individual animal data into groups.

However, becasue there is  temporal autocorrelation WITHIN individual animals, we need to use destructive sampling to make sure the groups are not temporally autocorrelated, so we remove data between groups for the time period in which temp AC exists based on the ACF (autocorrelation function).

This is called "destructive sampling" because you are actually throwing away data. The GEE approach DOES NOT impact the fit of the coefficent values, but only helps to accurately calculate the robust standard errors (by accounting for heteroskedasticity, i.e. differential variance among sub-populations)

After fitting a non-clustered model, we can use the residuals in the model to look at autocorrelation
We calculate the lag at which autocorrelation is no longer observed using acf.test
```{r}
acf.test <- function (residuals, id, type = c("correlation", "covariance","partial"), ci = 0.95)
{
  type <- match.arg(type)
  acfk <- lapply(levels(factor(id)), function(x) acf(residuals[id == x], type = type, plot = FALSE))
  threshold <- unlist(lapply(acfk, function(x) qnorm((1 + ci)/2)/sqrt(x$n.used)))
  lag <- unlist(lapply(1:length(acfk), function(i) which(acfk[[i]]$acf < threshold[i])[1]))
  return(list(acfk = acfk, threshold = threshold, lag = lag))
}

acf.test(m0$residuals,ssfdat.all$ID, type = "correlation")
```

Now we should use our ACF analysis to eliminate autocorrelation between clusters. Because our lag is 2, we could just split each puma into two-three equal sized samples with 2 points removed in between. Or you can split individuals by natural breaks in the data (still with the lag size removed). 

Buuuut for now (just to see how the model works) we're just going to make clusters by animal ID and year without eliminating the autocorrelated points (Don't do this in your analysis!)
```{r}
ssfdat.all %>% 
  mutate(year = year(t1_)) %>% 
  unite("clust_id_yr",c(ID,year),remove = FALSE) -> ssfdat.all
table(ssfdat.all$clust_id_yr)
```

Let's revisit our fixed effects models and add a cluster term
```{r}
m0_c <- clogit(case_ ~ road.dist.clean_end + rugged25.clean_end + vegetation.coarser.clean2_end + cluster(clust_id_yr) + 
               strata(stepID),method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
```

What is different about our model output in the model with clusters? (Nothing, because we didnt have the same individuals over multiple years)
```{r}
summary(m0)
summary(m0_c)
```

#### Model selection

Time for model comparison!
Instead of AIC, for conditional logistic regression models we use QIC
QIC is better suited to accommodate comparisons within strata while correcting
  for autocorrelation among strata
Here's our function for calculating QIC from an SSF model

```{r}
QIC.coxph <- function(mod, details = FALSE) {
  trace <- sum(diag(solve(mod$naive.var) %*% mod$var))
  quasi <- mod$loglik[2]
  return(-2*quasi + 2*trace)
}
```

Which model is best supported?
```{r}
QIC.coxph(m0)
QIC.coxph(m2)
```

Now we can assess model fit
Normally we would want ~ 100 repetitions, but in the interest of time we'll use a smaller number in class today
(Causing error... to troubleshoot)
```{r}
kfold.CV <- kfold(m0, k = 5, nrepet = 2, jitter = FALSE,
                  reproducible = TRUE, details = TRUE)
```

The correct validation value is just that of the observed points
```{r}
kfold.CV %>%
  group_by(type) %>%
  summarize(mean_cor = mean(cor))
```

#### Adding random effect

The second way to deal with correlated data is using random effects (like we did with RSFs)
The assumptions here are different! We don't need to have temporal autocorrelation within individuals but we expect that habitat selection varies among individuals. Unlike the GEE models that just correct your SEs, mixed-effects models will also change your coefficient estimates.

DID NOT UPDATE/TEST THIS

Random effects SSF with the coxme package
```{r}
m0_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end +
                 strata(stepID) + (1|ID),
               data = ssfdat.all, na.action = na.fail)
m1_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end + 
                ndvi_s_end:tod_start_ +
                strata(stepID) + (1|ID),
                data = ssfdat.all, na.action = na.fail)
m2_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end + 
                tri_s_end:log_sl +
                strata(stepID) + (1|ID),
                data = ssfdat.all, na.action = na.fail)
m3_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end + 
               ndvi_s_end:ndvi_s_start +
               strata(stepID) + (1|ID),
             data = ssfdat.all, na.action = na.fail)

summary(m0_me)

lrtest(m0_me,m1_me,m2_me,m3_me)
model.sel(m0_me,m1_me,m2_me,m3_me,rank=AIC)
```

Unfortunately there is no kfold command for coxme


The global model is a good start, expecially if we think animals respond similarly to their environment while moving
But, we might want to look at individual variation among animals in their movement behavior

So let's fit individual SSFs to each of our animals.
First, let's make a function for individual SSF models
```{r}
fitted_ssf <- function(issf_model){
  fit_issf(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + strata(stepID),method = "efron", robust = TRUE, data=issf_model)
}
```

Next, we can apply the conditional logistic regression model to our nested data
```{r}
ssfdat.all %>% 
  nest(-ID) %>% 
  mutate(mod = map(data, fitted_ssf)) -> m.ind
```

Package broom can help us tidy up out models into a tibble
```{r}
m.ind %>%
  mutate(tidy = map(mod, ~ broom::tidy(.x$model)),
         n = map_int(data, nrow)) -> m.ind
```

To vizualise our model output, we can reveal the estimates of all iSSFs
```{r}
m.ind$tidy
```
To vizualize or report the results, we may want to create a data frame with the 
  coefficients from all the SSFs
```{r}
ssf_coefs <- m.ind %>%
  unnest(tidy) 
```

Just to make it a little more interesting, we can add the sex of the animals for our vizualization
```{r}
n.covs <- 3
unique(m.ind$ID)
mutate(ssf_coefs, sex = c(rep(c("f","f","m","m","m","f","m","m","f"),each = n.covs))) -> ssf_coefs
```

Plot the coefficients!
```{r}
ssf_coefs %>% 
  ggplot(., aes(x=term, y=estimate, group = ID, col = factor(ID), pch = sex)) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),
                  position = position_dodge(width = 0.7), size = 0.8) +
  geom_hline(yintercept = 0, lty = 2)+
  facet_wrap(~term, scales="free") + 
  labs(x = "Covariate", y = "Relative Selection Strength") +
  theme_light()
```



