---
title: "Step Selection Functions"
author: "Kaitlyn"
date: "8/21/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

Explore Step Selection Functions for Hopland hunter data

## Owen tutorial
https://github.com/orbidder/rstatsclass/blob/master/Seminar%205%20SSFs/Seminar_5_SSFs.R

Today we'll be going over step-selection functions to model habitat selection. Specifically, we'll prepare the data, sample available locations, fit three different kinds of models, and assess model performance with cross-validation. We will talk about SSFs vs. iSSFs, fixed effects models vs. GEEs vs. mixed effects models, and how to think about autocorrelation and correlation within individuals.

Fundimentally, an SSF measures habitat selection along the movement path of an animal. In comparison to an RSF, it better represents actual selection decisions made by animals in real time, as they choose to move to specific locations out of a limited possibilies  available to them at each step

Load packages:
```{r}
# Syntax
library(tidyverse)
library(lubridate)
# Spatial
library(sf)
library(raster)
# New SSF package!
library(amt)
# Fitting conditional logistic regression models (in a Cox PH framework)
library(survival)
# Model selection using mixed-effects cox models
library(lmtest)
library(MuMIn)

# From hab package (in development)
# https://github.com/basille/hab/blob/master/R/kfold.r
library(devtools)
#install_github("basille/hab")
library(hab)
```

### Load in your environmental covariates

Unlike last week, when all our rasters were already in a stack, today we have 4 individual files. So we list the files and then bring them in as a stack using paste0
```{r}
#raster_files <- list.files("https://raw.githubusercontent.com/orbidder/rstatsclass/master/Seminar%205%20SSFs/",pattern = ".tif") 
#envtrasters <- raster::stack(paste0("Seminar%205%20SSFs/", raster_files))

# note - doesn't work when downloading remotely, just going to do it manually one by one
dem <- raster("https://raw.githubusercontent.com/orbidder/rstatsclass/master/Seminar%205%20SSFs/puma_dem.tif")
max_ndvi <- raster("https://raw.githubusercontent.com/orbidder/rstatsclass/master/Seminar%205%20SSFs/puma_ndvi.tif")
slope <- raster("https://raw.githubusercontent.com/orbidder/rstatsclass/master/Seminar%205%20SSFs/puma_slope.tif")
tri <- raster("https://raw.githubusercontent.com/orbidder/rstatsclass/master/Seminar%205%20SSFs/puma_tri.tif")

envtrasters <- raster::stack(dem, max_ndvi, slope, tri)

names(envtrasters) <- c("dem", "max_ndvi", "slope",  "tri")
```

Next, we'll bring in our GPS data, but we won't yet deal with making it spatial
```{r}
puma_tr1 <- read_csv("https://raw.githubusercontent.com/orbidder/rstatsclass/master/Seminar%205%20SSFs/puma_data_2015.csv") %>% 
  # First, select just the columns you need for the analysis
  dplyr::select(ID = animals_id, 5:7) %>% 
  # Then, make sure your date/times are in the correct time zone
  # Right now, the acquisition time is in UTC
  mutate(timestamp = lubridate::with_tz(acquisition_time,"America/Argentina/San_Juan")) 

# revert to old versions of unest/unnest
nest <- nest_legacy
unnest <- unnest_legacy

# To treat each animal differently, we will nest the data by animal ID
puma_tr1 <- puma_tr1 %>% tidyr::nest(-ID) 
# Check out the structure of the data
puma_tr1
```

Now we'll make a "track", which is used by package amt for movement analysis

This format helps amt to manage with variable fix rates and fix success so you don't have to!
```{r}
puma_tr1 <- puma_tr1 %>% 
  mutate(trk = map(data, function(d){
    mk_track(d, longitude, latitude, timestamp, crs = CRS("+init=epsg:4326")) %>% 
      # Transform our latlongs into UTMs
      transform_coords(CRS("+init=epsg:32719"))
  }))
```

### Correct non-normal fix rates

Are our fix rates normal? We should have steps 3 hours apart
```{r}
puma_tr1 %>%
  mutate(sr = lapply(trk, summarize_sampling_rate)) %>% 
  dplyr::select(ID, sr) %>%  unnest
```

Nope! So let's eliminate steps that are longer than 3 hours apart and filter out bursts that only have one point
```{r}
puma_tr1 %>% 
  mutate(steps = map(trk, function(x){
    x %>% 
      # Eliminate steps longer than 3 hours
      track_resample(rate = hours(3), tolerance = minutes(15)) %>%
      # Eliminate bursts with fewer than 2 points
      filter_min_n_burst(min_n = 2)})) -> ssfdat
```

Did we get rid of any locations? Compare the dimentions of our individual tracks in comparison to the steps we're keeping
```{r}
ssfdat
```

Check out the sampling rate of each of our study animals. Do the medians look about right? What do you notice about the max values? Take a close look at the means. Which animals do you think have lower fix success?
```{r}
ssfdat %>%
  mutate(sr = lapply(steps, summarize_sampling_rate)) %>% 
  dplyr::select(ID, sr) %>%  unnest
```


### Simulate steps & extract covariates

Next we simulate steps from our distribution of step lengths and turn angles, and extract the covariates for each step. 

A note on extracting covariates:
Generally we extract covariates at the END points of each step. However, in some cases, it may make sense to extract covariates ALONG a step. So, rather than ask: "did the animal end up in forest habitat?" you can ask: "what proportion of forest did they move through on their movement path?"

https://rdrr.io/cran/amt/man/extract_covariates.html

In other cases, it might make sense to extract covariates at the BEGINNING of the step
WHY??? 
As an interaction term - to see if the start location influences the end location or a movement parameter
  e.g. a habitat that is hard to move through
  e.g. due to group/herd effects
In the extract_covariates command, use where = "start", "end", or "both" depending on your goals

Here we make a function that will apply a bunch of commands to each of our individual animals. In this one piped function, we can sample random points, make a day/night covariate, and extract covariates!
```{r}
ssfdat %>%
  mutate(moddata = map(steps, function (x){
    x %>% 
      steps_by_burst() %>% 
      # Randomly sample 10 steps per real step
      random_steps(n = 10) %>% 
      # Determine day or night from movement track data
      time_of_day(where = "start") %>% 
      # Extract covariates from our raster stack
      amt::extract_covariates(envtrasters,where="both")})) -> ssfdat
```

We need to scale our covariates, but right now our data are in different tibbles by individual. 

If we want to scale the covariates from all the data, we need to make a single dataframe. To make one dataframe, we need a column for ID to tell the animals apart. To do that, we'll pull the IDs and the number of rows from each component of our data list
```{r}
ssfdat.all <- do.call(rbind,ssfdat$moddata)
ID<-c()
for (i in 1:length(ssfdat[[1]])) {
   id <- rep(ssfdat[[i,1]],dim(ssfdat$moddata[[i]])[1])
   ID<-c(ID,id)
}
ssfdat.all$ID <- ID

ssfdat.all 
```

One issue we have with our new dataset is that there are the same step IDs for multiple individual pumas. To deal with that, we'll make a new step column
```{r}
ssfdat.all$stepID <- ssfdat.all$ID*100000 + ssfdat.all$step_id_
```

We'll also remove any lines with NAs. Normally we shouldn't have NAs, but I cropped our raster layer too small, so this is just where we are!
```{r}
ssfdat.all <- drop_na(ssfdat.all)
```

Scale the covariates, turn "case" into a binary 1/0 variable, and add some movement parameters that we can use as covariates
```{r}
ssfdat.all %>% mutate(elev_s_start = scale(dem_start), 
                  slope_s_start = scale(slope_start),
                  tri_s_start = scale(tri_start),
                  ndvi_s_start = scale(max_ndvi_start),
                  elev_s_end = scale(dem_end), 
                  slope_s_end = scale(slope_end),
                  tri_s_end = scale(tri_end),
                  ndvi_s_end = scale(max_ndvi_end),
                  case_ = as.numeric(case_),
                  cos_ta = cos(ta_), 
                  log_sl = log(sl_)) -> ssfdat.all
```


### Data exploration

Before we run our models, let's visualize our step lengths and turn angles

Our turn angles are in radians.  For ease of vizualization, we'll turn them into degrees
```{r}
ssfdat.all %>%
  mutate(ta_degree = as_degree(ta_)) -> ssfdat.all
```

First, we can look at the distribution of our turning angles as a rose plot
```{r}
ssfdat.all %>% 
  filter(case_ == 1) %>% 
  ggplot(., aes(x = ta_degree, y=..density..)) + geom_histogram(breaks = seq(-180,180, by=20))+
  coord_polar(start = 0) + theme_minimal() + 
  scale_fill_brewer() + ylab("Density") + ggtitle("Angles Direct") + 
  scale_x_continuous("", limits = c(-180, 180), breaks = seq(-180, 180, by=20), 
                     labels = seq(-180, 180, by=20))+
  facet_wrap(~ID)
```

Also check out the distribution of step lengths
```{r}
ssfdat.all %>% 
  filter(case_ == 1) %>% 
  ggplot(., aes(x = sl_)) +  geom_histogram() + theme_minimal() + 
  scale_fill_brewer() + ylab("Count") + ggtitle("Step Lengths") +
  facet_wrap(~ID, scales="free")
```

We can also look at how movement parameters might differ between day and night
Here, we're looking at the log of step length
Why do we take the log of the step length? Modify the plot code to see!
```{r}
ssfdat.all %>% 
  filter(case_ == 1) %>% 
  ggplot(., aes(x = tod_start_, y = log(sl_))) + 
  geom_boxplot() + geom_smooth() + 
  facet_wrap(~ID)
```

Let's also look at what used and available steps look like
Here, the black point is our starting location and the colored points are our end locations
```{r}
ssfdat.all %>% 
  filter(step_id_ == 1, ID == 6) %>% 
  ggplot(.) + geom_point(aes(x = x2_,y = y2_,color = as.factor(case_))) + 
  geom_point(aes(x = x1_, y = y1_))
```

Check out some other strata to see how they look by modifying the ID and step_id_


A note on behaviors: You might imagine that animals select habitat really differently if they are in different behavioral states (i.e. resting, feeding, meandering, directed travel). Therefore, it is often wise to seperate your data into states before running SSF analyses.

You can determine the state of your animal in a few different ways. With just GPS data, you can fit a hidden markov model (HMM) to determine state with the moveHMM package. If you have fine-scale accelerometer data, you can also use that to determine state. 


### Run models

As with RSFs, start by writing out your candidate models.

Here are just a few examples of ways you can structure your models based on different hypotheses. Generally you will use a cluster() term any time you fit SSFs, particulalry if your data are autocorrelated. Therefore, these models below aren't really complete. Here we'll just check them out to get a rough idea of hypothesis testing and model fit

m0 just has habitat covariates at the end
note: amt has a wrapper for clogit called fit_issf. These commands should produce identical outcomes
```{r}
m0 <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               strata(stepID),method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
m0.b <- fit_issf(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               strata(stepID),method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)

summary(m0)
summary(m0.b)
```

Do animals select for ndvi differently between day and night? 
m1 has habitat covariates at the end with an interaction term between NDVI and time of day
```{r}
m1 <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               ndvi_s_end:tod_start_ +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)

```

Do animals select rugged habitats depending on their speed of movement?
m2 has habitat covariates the end with an interaction term between TRI at the start and movement rate
```{r}
m2 <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               tri_s_end:log_sl +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)

```

Are animals more likely to stay in the same NDVI as where they started?
```{r}
m3 <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               ndvi_s_end:ndvi_s_start +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
```

Let's look at our models
```{r}
summary(m0)
summary(m1)
summary(m2)
summary(m3)
```


### Dealing with correlated data

There are multiple ways to integrate correlated data into a single model. The first way is to build in a correlation structure in your model. We can do this by adding a cluster() parameter that seperates your data into clusters of correlated data.

The robust standard errors are now fit using generalized estimating equations (GEE), which correct for autocorrelated data. Note that using GGEs are only appropriate if your data are temporally autocorrelated (see Prima et al. 2017)

Important!!: For GGEs to do an adequate job fitting robust SEs, there need to be enough clusters (>30) (see Prima et al. 2017). Therefore, when we use cluster(), we might want to seperate individual animals into multiple clusters by breaking individual animal data into groups.

However, becasue there is  temporal autocorrelation WITHIN individual animals, we need to use destructive sampling to make sure the groups are not temporally autocorrelated, so we remove data between groups for the time period in which temp AC exists based on the ACF (autocorrelation function).

This is called "destructive sampling" because you are actually throwing away data. The GEE approach DOES NOT impact the fit of the coefficent values, but only helps to accurately calculate the robust standard errors (by accounting for heteroskedasticity, i.e. differential variance among sub-populations)

After fitting a non-clustered model, we can use the residuals in the model to look at autocorrelation
We calculate the lag at which autocorrelation is no longer observed using acf.test
```{r}
acf.test <- function (residuals, id, type = c("correlation", "covariance","partial"), ci = 0.95)
{
  type <- match.arg(type)
  acfk <- lapply(levels(factor(id)), function(x) acf(residuals[id == x], type = type, plot = FALSE))
  threshold <- unlist(lapply(acfk, function(x) qnorm((1 + ci)/2)/sqrt(x$n.used)))
  lag <- unlist(lapply(1:length(acfk), function(i) which(acfk[[i]]$acf < threshold[i])[1]))
  return(list(acfk = acfk, threshold = threshold, lag = lag))
}

acf.test(m0$residuals,ssfdat.all$ID, type = "correlation")
```

Now we should use our ACF analysis to eliminate autocorrelation between clusters. Because our lag is 2, we could just split each puma into two-three equal sized samples with 2 points removed in between. Or you can split individuals by natural breaks in the data (still with the lag size removed). 

Buuuut for now (just to see how the model works) we're just going to make clusters by animal ID and year without eliminating the autocorrelated points (Don't do this in your analysis!)
```{r}
ssfdat.all %>% 
  mutate(year = year(t1_)) %>% 
  unite("clust_id_yr",c(ID,year),remove = FALSE) -> ssfdat.all
table(ssfdat.all$clust_id_yr)
```

Let's revisit our fixed effects models and add a cluster term
```{r}
m0_c <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + cluster(clust_id_yr) +
               strata(stepID),method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
m1_c <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + ndvi_s_end:tod_start_ + cluster(clust_id_yr) +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
m2_c <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               tri_s_end:log_sl + cluster(clust_id_yr) +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
m3_c <- clogit(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + 
               ndvi_s_end:ndvi_s_start + cluster(clust_id_yr) +
               strata(stepID), method = "efron", robust = TRUE, data = ssfdat.all, model = TRUE)
```

What is different about our model output in the model with clusters?
```{r}
summary(m0)
summary(m0_c)
```

### Model selection

Time for model comparison!
Instead of AIC, for conditional logistic regression models we use QIC
QIC is better suited to accommodate comparisons within strata while correcting
  for autocorrelation among strata
Here's our function for calculating QIC from an SSF model

```{r}
QIC.coxph <- function(mod, details = FALSE) {
  trace <- sum(diag(solve(mod$naive.var) %*% mod$var))
  quasi <- mod$loglik[2]
  return(-2*quasi + 2*trace)
}
```

Which model is best supported?
```{r}
QIC.coxph(m0_c)
QIC.coxph(m1_c)
QIC.coxph(m2_c)
QIC.coxph(m3_c)
```

Now we can assess model fit
Normally we would want ~ 100 repetitions, but in the interest of time we'll use a smaller number in class today
```{r}
kfold.CV <- kfold(m0_c, k = 5, nrepet = 2, jitter = FALSE,
                  reproducible = TRUE, details = TRUE)
```

The correct validation value is just that of the observed points
```{r}
kfold.CV %>%
  group_by(type) %>%
  summarize(mean_cor = mean(cor))
```

### Adding random effect

The second way to deal with correlated data is using random effects (like we did with RSFs)
The assumptions here are different! We don't need to have temporal autocorrelation within individuals but we expect that habitat selection varies among individuals. Unlike the GEE models that just correct your SEs, mixed-effects models will also change your coefficient estimates.

Random effects SSF with the coxme package
```{r}
m0_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end +
                 strata(stepID) + (1|ID),
               data = ssfdat.all, na.action = na.fail)
m1_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end + 
                ndvi_s_end:tod_start_ +
                strata(stepID) + (1|ID),
                data = ssfdat.all, na.action = na.fail)
m2_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end + 
                tri_s_end:log_sl +
                strata(stepID) + (1|ID),
                data = ssfdat.all, na.action = na.fail)
m3_me<-coxme(Surv(rep(1, length(ssfdat.all$ID)), case_) ~ elev_s_end + tri_s_end + ndvi_s_end + 
               ndvi_s_end:ndvi_s_start +
               strata(stepID) + (1|ID),
             data = ssfdat.all, na.action = na.fail)

summary(m0_me)

lrtest(m0_me,m1_me,m2_me,m3_me)
model.sel(m0_me,m1_me,m2_me,m3_me,rank=AIC)
```

Unfortunately there is no kfold command for coxme


The global model is a good start, expecially if we think animals respond similarly to their environment while moving
But, we might want to look at individual variation among animals in their movement behavior

So let's fit individual SSFs to each of our animals.
First, let's make a function for individual SSF models
```{r}
fitted_ssf <- function(issf_model){
  fit_issf(case_ ~ elev_s_end + tri_s_end + ndvi_s_end + strata(stepID),method = "efron", robust = TRUE, data=issf_model)
}
```

Next, we can apply the conditional logistic regression model to our nested data
```{r}
ssfdat.all %>% 
  nest(-ID) %>% 
  mutate(mod = map(data, fitted_ssf)) -> m.ind
```

Package broom can help us tidy up out models into a tibble
```{r}
m.ind %>%
  mutate(tidy = map(mod, ~ broom::tidy(.x$model)),
         n = map_int(data, nrow)) -> m.ind
```

To vizualise our model output, we can reveal the estimates of all iSSFs
```{r}
m.ind$tidy
```
To vizualize or report the results, we may want to create a data frame with the 
  coefficients from all the SSFs
```{r}
ssf_coefs <- m.ind %>%
  unnest(tidy) 
```

Just to make it a little more interesting, we can add the sex of the animals for our vizualization
```{r}
n.covs <- 3
unique(m.ind$ID)
mutate(ssf_coefs, sex = c(rep(c("f","f","m","m","m","f","m","m","f"),each = n.covs))) -> ssf_coefs
```

Plot the coefficients!
```{r}
ssf_coefs %>% 
  ggplot(., aes(x=term, y=estimate, group = ID, col = factor(ID), pch = sex)) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),
                  position = position_dodge(width = 0.7), size = 0.8) +
  geom_hline(yintercept = 0, lty = 2)+
  facet_wrap(~term, scales="free") + 
  labs(x = "Covariate", y = "Relative Selection Strength") +
  theme_light()
```



## Vignette from CRAN

https://cran.r-project.org/web/packages/amt/vignettes/p4_SSF.html

This vignette briefly introduces how one can fit a Step-Selection Function (SSF) with the `amt` package. We will be using the example data of one red deer from northern Germany and one covariate: a forest cover map.

### Getting the data ready

First we load the required libraries and the relocation data (called deer)

```{r}
library(lubridate)
library(raster)
library(amt)
data("deer")
deer
```

In order to continue, we need a regular sampling rate. To check the current sampling rate, we use `summarize_sampling_rate`:

```{r}
summarize_sampling_rate(deer)
```

The median sampling rate is 6h, which is what we aimed for.

Next, we have to get the environmental covariates. A forest layer is included in the package. Note, that this a regular RasterLayer.

```{r}
data("sh_forest")
sh_forest
```


### Prepare Data for SSF

#### Steps

Before fitting a SSF we have to do some data preparation. First, we change from a point representation to a step representation, using the function `steps_by_burst`, which in contrast to the `steps` function accounts for bursts.

```{r}
ssf1 <- deer %>% steps_by_burst()
```

#### Control/random steps

The generic function `random_steps` provides a methods for a `track_xy*`, where each observed step is paired with `n_control` control steps (i.e., steps that share the same starting location but have different turn angles and step lengths). The distributions for drawing step lengths and turning angles are usually obtained by fitting known parametric distribution to the observed step length and turn angles.

The function `random_steps` has seven arguments. For most use cases the defaults are just fine, but there might situation where the user wants to adjust some of the arguments. The arguments are:

`x`: This is the `track_xy*` for which the random steps are created. That is, for each step in `x` `n_control` random steps are created.
`n_control`: The number of random steps that should be created for each observed step.
`sl_distr`: This is the distribution of the step lengths. By default a gamma distribution is fit to the observed step lengths of the x. But any `amt_distr` is suitable here. 1
`ta_distr`: This is the turn angle distribution, with the default being a von Mises distribution.
`rand_sl`: These are the random step lengths, by default 1e5 random numbers from the distribution fitted in 3.
`rand_ta`: These are the random turn angles, by default 1e4 random numbers from the distribution fitted in 4.
include_observed: This argument is by default TRUE and indicates if the observed steps should be included or not.

In most situations the following code snippet should work.
```{r}
ssf1 <- ssf1 %>% random_steps(n_control = 15)
```

#### Extract covariates

As a last step, we have to extract the covariates at the end point of each step. We can do this with `extract_covariates.`

```{r}
ssf1 <- ssf1 %>% extract_covariates(sh_forest) 
```

Since the forest layers is coded as `1 = forest` and `2 != forest`, we create a factor with appropriate levels. We also calculate the log of the step length and the cosine of the turn angle, which we may use later for a integrated step selection function.

```{r}
ssf1 <- ssf1 %>% 
  mutate(forest = factor(sh.forest, levels = 1:2, labels = c("forest", "non-forest")), 
         cos_ta = cos(ta_), 
         log_sl = log(sl_)) 
```


### Fitting SSF

Now all pieces are there to fit a SSF. We will use `fit_clogit`, which is a wrapper around `survival::clogit`.

```{r}
m0 <- ssf1 %>% fit_clogit(case_ ~ forest + strata(step_id_))
m1 <- ssf1 %>% fit_clogit(case_ ~ forest + forest:cos_ta + forest:log_sl + log_sl * cos_ta + strata(step_id_))
m2 <- ssf1 %>% fit_clogit(case_ ~ forest + forest:cos_ta + forest:log_sl + log_sl + cos_ta + strata(step_id_))

summary(m0)
summary(m1)
summary(m2)

AIC(m0$model)
AIC(m1$model)
AIC(m2$model)
```



## Example from Taal Levi
```{r}
##############################################X
#--------WILD 6900 -- Space-use Ecology-------X
#--Lab 9: Integrated Step Selection Analysis--X
##############################################X

#In this lab we will perform an integrated step selection
  #analysis (iSSA). As we saw in class, the iSSA is a 
  #mechanistic model of habitat selection and movement.

#The iSSA was introduced in Avgar et al. 2016, and 
  #Appendix 4 of that paper is an iSSA practical user guide.
  #In this lab, we'll follow that guide to make it clear which
  #functions from 'amt' correspond to which steps of the iSSA.

#After fitting the iSSF, we'll use log-RSS to quantify habitat 
 #selection.

#For this lab, make sure you have the latest version of 'amt'
  #from CRAN, which is currently version 0.0.9.

# install.packages("amt")

#You'll also need the package "patchwork" for the very last
  #plot we'll make

# install.packages("patchwork")

#Set options----
options(stringsAsFactors = FALSE)

#Load packages----
library(tidyverse)
library(raster)
library(amt)
library(patchwork)

#The code sections that follow are from Appendix S4 of Avgar et al. 2016

#1. Collect animal positional data----
#Here, we'll load our unicorn data. Each iSSF will be fit to a
  #single individual's location data. We could fit the model for
  #all of our individuals and then summarize if we are interested
  #in population-level inference, but for now, a mixed-effects 
  #approach is not easy for an iSSA (but see Muff et al. 2019).

#For that reason, we'll just work with a single individual here.
  #You could use a for loop, an lapply(), or a nested data.frame
  #to repeat this for every individual, but for clarity, we'll 
  #focus on just one.

unicorn <- readRDS("Data/from tutorial/unicorn_gps_data.rds")

#Let's arbitrarily pick unicorn #2 for this
uni <- unicorn %>% 
  filter(id == 2)

#We also need our landscape covariates!
landscape <- brick("Data/from tutorial/unicorn_landscape_data.grd")

#2. Tabulate observed steps----
#Here is where you would likely want to clean your data to remove
  #erroneous fixes. Our unicorn data is already clean, so we
  #can move on.

#We've already seen the function make_track() in 'amt' to prep our 
  #data. From there, we can make our points into steps with the
  #function steps(). This does all the tabulation we need!
uni_steps <- uni %>% 
  make_track(.x = x, .y = y, .t = dt, crs = CRS("+init=epsg:32612")) %>% 
  steps()

#Let's check the result:
head(uni_steps)

#You can see that the function did all the tabulation we need.
  #We have the starting (x1_, y1_) and ending (x2_, y2_) points
  #of each step, the step length (sl_), the turning angle (ta_),
  #the start and end times (t1_ and t2_), and the duration
  #of the step (dt_).

#Remember, all steps should have the same duration. If they do not,
  #'amt' has functions you can use to resample your track before
  #moving on.

#3 - 5. Sample available steps----

#For each observed step, we'll sample 15 available steps.

#You can choose which distributions you want to use to generate your
  #available steps, but here will we use the gamma (step lengths) and 
  #von Mises (turning angles).

#You have to "guess" the parameters of these distributions to sample
  #your steps. Remember, we'll adjust this guess with the fitted
  #model parameters. But a good starting point is to simply fit
  #the distributions to your observed data. 'amt' will do this for
  #you automatically when you sample random steps (it's the default),
  #but we'll write it out here to be clear where those numbers came from.

# original code gives error... ntried removing fit_distr from in there
set.seed(123456)
#uni_rand <- uni_steps %>% 
#  random_steps(n_control = 15,
#               sl_distr = fit_distr(uni_steps$sl_, "gamma"),
#               ta_distr = fit_distr(uni_steps$ta_, "vonmises"))

uni_rand <- uni_steps %>% 
  random_steps(n_control = 15,
               sl_distr = "gamma",
               ta_distr = "vonmises")

#6. Attach step attributes----
#Now we attach our covariates to each step. We've already seen 
  #the 'amt' function 'extract_covariates()', but when applied
  #to steps, we have extra options. We can get the values of
  #the raster at the end point (the default) or the start point
  #of the step... or both! There is also a function that will
  #extract the covariate along the step 'extract_covariates_along()',
  #which generally needs more processing.

#Here, we'll just use the endpoint.
uni_hab <- uni_rand %>% 
  extract_covariates(landscape, where = "end")

#We also need the movement parameters to include in our model.
  #We already have 'sl_' and 'ta_', but remember, for the gamma
  #distribution, we also need the log of the step length. For the
  #von Mises distribution, we also need the cosine of the turn
  #angle.
uni_hab <- uni_hab %>% 
  mutate(log_sl_ = log(sl_),
         cos_ta_ = cos(ta_))

#Let's see what we have so far
View(uni_hab)

#7. Fit conditional logistic regression----
#We're ready to fit our model. Don't forget to include the
  #movement parameters! And don't forget to include the
  #step IDs as the strata!

issf <- uni_hab %>% 
  fit_issf(case_ ~ habitat + elevation + I(elevation^2) +
             sl_ + log_sl_ + cos_ta_ + 
             strata(step_id_),
           model = TRUE)

#Note the argument 'model = TRUE'. We need this option to be 
  #passed to survival::clogit in order for predict() to work
  #later (when we calculate log-RSS)

#View summary
summary(issf)

#8. Adjust movement coefficients----
#Now we can take the movement coefficients from the model and 
  #update our original distributions. See the "cheatsheet" fro
  #the lecture with how to do this in general. 

#We have a function in 'amt' that will give us the original parameters
sl_distr_params(issf) # doesn't work...

#Store them for convenience
old_shape <- sl_distr_params(issf)$shape
old_scale <- sl_distr_params(issf)$scale

#Let's also grab the betas
beta_log_sl_ <- coef(issf)["log_sl_"]
beta_sl_ <- coef(issf)["sl_"]

#Now update
new_shape <- old_shape + beta_log_sl_
new_scale <- 1/(1/old_scale - beta_sl_)

#Same idea for turning angle (a little more concise)
new_conc <- ta_distr_params(issf)$kappa + coef(issf)["cos_ta_"]

#This is likely negative because we fixed the mean to 0. We don't need to
  #worry about this right now.

#9. Simulate space use----
#This last step is very complex. There are functions in 'amt'
  #that Johannes Signer is developing to help you do this,
  #but they are still experimental and are tricky to use.

#If you're interested, you can start by looking here and playing
  #with these functions. Expect more updates in the near future.
?simulate_ud
?dispersal_kernel

#log-RSS----
#We've already seen how to calculate relative selection strength
  #from a fitted RSF. We also saw in our last lecture that the
  #approach is exactly the same for an iSSF, the interpretation
  #is just different. Remember, all of our steps in a stratum
  #have the same starting location, so we're now comparing the
  #ratio of two steps with the same starting point that differ
  #only in their endpoint.

#Also recall, that log-RSS is the correct interpretation of
  #our beta parameters. That is, the log-RSS for a two steps
  #that are identical except that one ends at elevation 1 and
  #the other ends at elevation 0 (i.e., delta(elevation) = 1)
  #is just beta_elevation:
coef(issf)["elevation"]

#For a more complex change, we can use the predict() function
  #to get our log-RSS, just as we saw for the RSF case.

#Check the class of our model
class(issf) #This is an 'amt' class

#Turns out, hiding inside is the actual model fit
class(issf$model)

#There is our actual conditional logistic regression model,
  #which was actual fit by a "trick" using a cox proportional
  #hazards model (class "coxph"). So if you need help, the
  #predict function is 
?survival::predict.coxph

#While it's useful to know how to do this yourself, we now
  #have a function in 'amt' that will calculate log-RSS for
  #you using this method.
?log_rss

#In the help file, you can see an example for a fitted SSF.
  #As with RSFs, we still have to define x1 and x2. Since
  #we're taking advantage of the predict() method, we need to
  #define all model variables for x1 and x2 (including the
  #movement parameters). Remember, the covariates we aren't
  #trying to draw inference from will cancel out if they are
  #equal in x1 and x2, so it doesn't matter what they are.

#We will define x1 as a range of hypothetical locations, and 
  #we'll use x2 as a single reference location.
x1 <- data.frame(elevation = rep(seq(0, 1, length.out = 50), 2),
                 habitat = rep(c(0, 1), each = 50),
                 sl_ = 1, log_sl_ = log(1), cos_ta_ = cos(0))
x2 <- data.frame(elevation = 0.5,
                 habitat = 0,
                 sl_ = 1, log_sl_ = log(1), cos_ta_ = cos(0))

#Calculate log-RSS
logRSS <- log_rss(issf, x1, x2)

#There's a default plot() method for a simple figure
plot(logRSS)

#If we take a closer look, we can see how to plot using ggplot

#Inspect the object
str(logRSS)

#Here's what we want to plot
logRSS$df

#Plot it!
rss_plot2 <- ggplot(logRSS$df, aes(x = elevation_x1, y = log_rss, color = factor(habitat_x1))) +
  geom_hline(yintercept = 0) +
  geom_line(size = 1) +
  xlab("Elevation (x1)") +
  ylab("log-RSS") +
  ggtitle("Unicorn 2") +
  scale_color_discrete(name = "Habitat (x1)") + 
  theme_bw()
rss_plot2

#Piped workflow----
#The package 'amt' was meant to make this sort of thing easy to do
  #in a piped workflow. Let's fit a model for unicorn #3 to demonstrate.

set.seed(654321)
issf3 <- unicorn %>% 
  filter(id == 3) %>% 
  make_track(.x = x, .y = y, .t = dt, crs = CRS("+init=epsg:32612")) %>% 
  steps() %>% 
  random_steps(n_control = 15) %>% 
  extract_covariates(landscape, where = "end") %>% 
  mutate(log_sl_ = log(sl_),
         cos_ta_ = cos(ta_)) %>% 
  fit_issf(case_ ~ habitat + elevation + I(elevation^2) +
             sl_ + log_sl_ + cos_ta_ + 
             strata(step_id_),
           model = TRUE)

#Summary
summary(issf3)

#log-RSS
logRSS3 <- log_rss(issf3, x1, x2)

#Plot
rss_plot3 <- ggplot(logRSS3$df, aes(x = elevation_x1, y = log_rss, color = factor(habitat_x1))) +
  geom_hline(yintercept = 0) +
  geom_line(size = 1) +
  xlab("Elevation (x1)") +
  ylab("log-RSS") +
  ggtitle("Unicorn 3") +
  scale_color_discrete(name = "Habitat (x1)") + 
  theme_bw()
rss_plot3

#Combine (using package patchwork: https://github.com/thomasp85/patchwork)
rss_plot2 / rss_plot3

```

